# Regularisation
[[Advanced regularisation for neural networks]]
[Types of Regularization in Machine Learning | by Aqeel Anwar | Feb, 2021 | Towards Data Science](https://towardsdatascience.com/types-of-regularization-in-machine-learning-eb5ce5f9bf50)

Q. What does regularisation mean in ML?
A. Modifying the objective function to penalise high complexity

Q. What is the objective of regularisation in ML?
A. Decrease overfitting by decreasing model complexity

Q. What separates L1 and L2 regularisation?
A. L1 is linear penalty, L2 is squared penalty

Q. What advantages does L1 hold over L2 regularisation?
A. Typically induces feature selection

Q. When would we choose L2 over L1 regularisation?
A. When model performance is more important than sparsity/parsimony.

Q. What is a synonym for L1-regularisation?
A. Lasso

Q. What is lasso-regularisation a synonym for?
A. L1 regularisation

Q. What is a synonym for L2-regularisation?
A. Ridge regularization 

Q. What is ridge-regularisation a synonym for?
A. L2 regularisation

Q. Why does L2-regularisation result in better model performance than L1?
A. L2 regularisation allows parameters to be near-0, while L1 typically reduces to 0

Q. What does *precision* in ML translate to in diagnostic epi?
A. PPV

Q. What does *recall* in ML translate to in diagnostic epi?
A. Sensitivity

Q. How do we define *accuracy* in ML? 
A. Ratio of true classifications to all classifications

Q. In which cases is accuracy a helpful metric for ML?
A. When all correct predictions are equally important

## Backlinks
* [[Overfitting]]
	* Q. How might we counter overfitting in ML?
	* [[Regularisation]]

<!-- #anki/deck/ML -->

<!-- {BearID:A7F19937-FD72-44E0-B487-49E4F7435003-5010-000010D6D617D673} -->
