# Statistics can be dangerous
Since they can allow for [[Selective criticisms create bias]].

A list of all kinds of problems:
* “spurious correlations” caused by data processing (such as ratio/percentage data, or normalizing to a common time-series)
* multiplicity: many subgroups or hypotheses tested, with only statistically-significant ones reported and no control of the overall false detection rate
* missingness not modeled
* in vivo animal results applied to humans
* experiment run on regular children rather than identical twins
* publication bias detected in meta-analysis
* a failure to reject the null or a positive point-estimate being interpreted as evidence for the null hypothesis
* “the difference between statistically-significant and non-statistically-significant is not statistically-significant”
* choice of an inappropriate distribution, like modeling a log-normal variable by a normal variable (“they strain at the gnat of the prior who swallow the camel of the likelihood”)
* no use of single or double-blinding or placebos
* a genetic study testing correlation between 1 gene and a trait
* an IQ experiment finding an intervention increased before/after scores on some IQ subtests and thus increased IQ
* cross-sectional rather than longitudinal study
* ignoring multilevel structure (like data being collected from sub-units of schools, countries, families, companies, websites, individual fishing vessels, WP editors etc)
* reporting performance of GWAS polygenic scores using only SNPs which reach genome-wide statistical-significance
* nonzero attrition but no use of intent-to-treat analysis
* use of a fixed alpha threshold like 0.05
* correlational data interpreted as causation
* use of an “unidentified” model, requiring additional constraints or priors
* non-preregistered analyses done after looking at data; p-hacking of every shade
* use of cause-specific mortality vs all-cause mortality as a measurement
* use of measurements with high levels of measurement error (such as dietary questionnaires)
* ceiling/floor effects (particularly IQ tests)
* claims about latent variables made on the basis of measurements of greatly differing quality
* or after “controlling for” intermediate variables, comparing total effects of one variable to solely indirect effects of another
* or that one variable mediates an effect without actually setting up a mediation SEM
* studies radically underpowered to detect a plausible effect
* the “statistical-significance filter” inflating effects
* base rate fallacy
* self-selected survey respondents; convenience samples from Mechanical Turk or Google Surveys or similar services
* animal experiments with randomization not blocked by litter/cage/room
* using a large dataset and obtaining many statistically-significant results
* factor analysis without establishing measurement invariance
* experimenter demand effects
* using a SVM/NN/RF without crossvalidation or heldout sample
* using them, but with data preprocessing done or hyperparameters selected based on the whole dataset
* passive control groups
* not doing a factorial experiment but testing one intervention on each group
* flat priors overestimating effects
* reporting of relative risk increase without absolute risk increase
* a genetic study testing correlation between 500,000 genes and a trait
* conflicts of interest by the researchers/funders
* lack of power analysis to design experiment
* analyzing Likert scales as a simple continuous cardinal variable
* animal results in a single inbred or clonal strain, with the goal of reducing variance/increased power (Michie 1955)
* right-censored data
* temporal autocorrelation of measurements
* genetic confounding
* reliance on interaction terms

## Backlinks
* [[How should we critique research]]
	* [[Statistics can be dangerous]]

<!-- #Work -->

<!-- {BearID:04D1208C-8B27-4C3D-9188-077EF23E00D4-15756-0000130BF52A7B81} -->
